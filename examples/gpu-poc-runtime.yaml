apiVersion: v1
kind: Service
metadata:
  name: nccl-host-1
spec:
  selector:
    name: nccl-host-1
  clusterIP: None
---
apiVersion: v1
kind: Service
metadata:
  name: nccl-host-2
spec:
  selector:
    name: nccl-host-2
  clusterIP: None
---
apiVersion: v1
kind: Pod
metadata:
  name: nccl-host-1
  labels:
    name: nccl-host-1
  annotations:
    networking.gke.io/default-interface: 'eth0'
    networking.gke.io/interfaces: |
      [
        {"interfaceName":"eth0","network":"default"},
        {"interfaceName":"eth2","network":"rdma-0"},
        {"interfaceName":"eth3","network":"rdma-1"},
        {"interfaceName":"eth4","network":"rdma-2"},
        {"interfaceName":"eth5","network":"rdma-3"},
        {"interfaceName":"eth6","network":"rdma-4"},
        {"interfaceName":"eth7","network":"rdma-5"},
        {"interfaceName":"eth8","network":"rdma-6"},
        {"interfaceName":"eth9","network":"rdma-7"}
      ]
    kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
    gke-gcsfuse/volumes: "true"
    gke-gcsfuse/cpu-limit: "0"
    gke-gcsfuse/memory-limit: "0"
    gke-gcsfuse/ephemeral-storage-limit: "0"
spec:
  hostNetwork: false
  hostPID: false
  serviceAccount: workload-identity-k8s-sa
  volumes:
    - name: library-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: gib
      hostPath:
        path: /home/kubernetes/bin/gib
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 250Gi
    - name: checkpoint-volume
      persistentVolumeClaim:
        claimName: checkpoint-gke-a4-poc-c6aa9adf-pvc
    - name: training-volume
      persistentVolumeClaim:
        claimName: training-gke-a4-poc-50421fa9-pvc
  containers:
    - image: nvcr.io/nvidia/pytorch:25.01-py3
      name: ngc-25-01
      resources:
        requests:
          cpu: 150m
      volumeMounts:
        - name: library-dir-host
          mountPath: /usr/local/nvidia
        - name: gib
          mountPath: /usr/local/gib
        - name: shared-memory
          mountPath: /dev/shm
        - name: checkpoint-volume
          mountPath: /checkpoint_data
        - name: training-volume
          mountPath: /cfs

      env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
        - name: N_NODES
          value: "2"
      resources:
        limits:
          nvidia.com/gpu: 8
      command: ["/bin/bash", "-c"]
      args:
        - |
          cp -R /dev/shm/scripts /
          cp -R /dev/shm/diagnostic /
          cp -R /dev/shm/third_party /

          export N_NODES=${N_NODES}

          # Load all the cuda libs
          /sbin/ldconfig

          # Install ping
          apt update -y
          apt install -y iputils-ping openssh-server iproute2 pciutils

          # Setup ssh
          cp /cfs/id_rsa* /cfs/authorized_keys /root/.ssh/
          chmod 400 /root/.ssh/id_rsa
          sed -i 's/^#PermitRootLogin prohibit-password/PermitRootLogin prohibit-password/' /etc/ssh/sshd_config
          echo "Port 222" | tee -a /etc/ssh/sshd_config > /dev/null
          service ssh restart

          # Setup variables
          echo "unset NCCL_NVLS_ENABLE TORCH_NCCL_USE_COMM_NONBLOCKING" | tee -a /root/.bashrc > /dev/null
          echo "export LD_LIBRARY_PATH=/usr/local/gib/lib64:${LD_LIBRARY_PATH}" | tee -a /root/.bashrc > /dev/null
          echo "source /usr/local/gib/scripts/set_nccl_env.sh" | tee -a /root/.bashrc > /dev/null

          #
          python -m pip install --upgrade pip
          pip install sentencepiece

          # Get helper variables to form all hostnames
          export POSTFIX=$(hostname | cut -d . -f 2-)
          export WORKERS_BASENAME=$(hostname | cut -d . -f 1 | rev | cut -d - -f 2- | rev )

          # For every worker, wait till online and add to hostfile
          for i in `seq 1 $(($N_NODES))`; do
            OTHER=${WORKERS_BASENAME}-${i}
            until ssh -p 222 -o StrictHostKeyChecking=no $OTHER hostname; do
              echo Waiting for ${OTHER}...
              sleep 10
            done
            echo ${OTHER} port=222 slots=8 | tee -a /tmp/hostfile;
          done

          sleep infinity
  initContainers:
    - name: nccl-plugin-installer
      image: us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-diagnostic:v1.0.5
      imagePullPolicy: Always
      args:
      - |
        set -ex
        /scripts/container_entry.sh install --install-nccl
        cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
        cp -R /var/lib/gib/. /target/usr/local/gib
        cp -R /scripts /target/dev/shm
        cp -R /diagnostic /target/dev/shm
        cp -R /third_party /target/dev/shm
      command:
      - /bin/sh
      - -c
      volumeMounts:
      - mountPath: /target/usr/local/gib
        name: gib
      - mountPath: /target/dev/shm
        name: shared-memory

---
apiVersion: v1
kind: Pod
metadata:
  name: nccl-host-2
  labels:
    name: nccl-host-2
  annotations:
    networking.gke.io/default-interface: 'eth0'
    networking.gke.io/interfaces: |
      [
        {"interfaceName":"eth0","network":"default"},
        {"interfaceName":"eth2","network":"rdma-0"},
        {"interfaceName":"eth3","network":"rdma-1"},
        {"interfaceName":"eth4","network":"rdma-2"},
        {"interfaceName":"eth5","network":"rdma-3"},
        {"interfaceName":"eth6","network":"rdma-4"},
        {"interfaceName":"eth7","network":"rdma-5"},
        {"interfaceName":"eth8","network":"rdma-6"},
        {"interfaceName":"eth9","network":"rdma-7"}
      ]
    kueue.x-k8s.io/podset-preferred-topology: "kubernetes.io/hostname"
    gke-gcsfuse/volumes: "true"
    gke-gcsfuse/cpu-limit: "0"
    gke-gcsfuse/memory-limit: "0"
    gke-gcsfuse/ephemeral-storage-limit: "0"
spec:
  hostNetwork: false
  hostPID: false
  serviceAccount: workload-identity-k8s-sa
  volumes:
    - name: library-dir-host
      hostPath:
        path: /home/kubernetes/bin/nvidia
    - name: gib
      hostPath:
        path: /home/kubernetes/bin/gib
    - name: shared-memory
      emptyDir:
        medium: "Memory"
        sizeLimit: 250Gi
    - name: checkpoint-volume
      persistentVolumeClaim:
        claimName: checkpoint-gke-a4-poc-c6aa9adf-pvc
    - name: training-volume
      persistentVolumeClaim:
        claimName: training-gke-a4-poc-50421fa9-pvc
  containers:
    - image: nvcr.io/nvidia/pytorch:25.01-py3
      name: ngc-25-01
      resources:
        requests:
          cpu: 150m
      volumeMounts:
        - name: library-dir-host
          mountPath: /usr/local/nvidia
        - name: gib
          mountPath: /usr/local/gib
        - name: shared-memory
          mountPath: /dev/shm
        - name: checkpoint-volume
          mountPath: /checkpoint_data
        - name: training-volume
          mountPath: /cfs

      env:
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
        - name: N_NODES
          value: "2"
      resources:
        limits:
          nvidia.com/gpu: 8
      command: ["/bin/bash", "-c"]
      args:
        - |
          cp -R /dev/shm/scripts /
          cp -R /dev/shm/diagnostic /
          cp -R /dev/shm/third_party /

          export N_NODES=${N_NODES}

          # Load all the cuda libs
          /sbin/ldconfig

          # Install ping
          apt update -y
          apt install -y iputils-ping openssh-server iproute2 pciutils

          # Setup ssh
          cp /cfs/id_rsa* /cfs/authorized_keys /root/.ssh/
          chmod 400 /root/.ssh/id_rsa
          sed -i 's/^#PermitRootLogin prohibit-password/PermitRootLogin prohibit-password/' /etc/ssh/sshd_config
          echo "Port 222" | tee -a /etc/ssh/sshd_config > /dev/null
          service ssh restart

          # Setup variables
          echo "unset NCCL_NVLS_ENABLE TORCH_NCCL_USE_COMM_NONBLOCKING" | tee -a /root/.bashrc > /dev/null
          echo "export LD_LIBRARY_PATH=/usr/local/gib/lib64:${LD_LIBRARY_PATH}" | tee -a /root/.bashrc > /dev/null
          echo "source /usr/local/gib/scripts/set_nccl_env.sh" | tee -a /root/.bashrc > /dev/null

          #
          python -m pip install --upgrade pip
          pip install sentencepiece

          # Get helper variables to form all hostnames
          export POSTFIX=$(hostname | cut -d . -f 2-)
          export WORKERS_BASENAME=$(hostname | cut -d . -f 1 | rev | cut -d - -f 2- | rev )

          # For every worker, wait till online and add to hostfile
          for i in `seq 1 $(($N_NODES))`; do
            OTHER=${WORKERS_BASENAME}-${i}
            until ssh -p 222 -o StrictHostKeyChecking=no $OTHER hostname; do
              echo Waiting for ${OTHER}...
              sleep 10
            done
            echo ${OTHER} port=222 slots=8 | tee -a /tmp/hostfile;
          done

          sleep infinity
  initContainers:
    - name: nccl-plugin-installer
      image: us-docker.pkg.dev/gce-ai-infra/gpudirect-gib/nccl-plugin-gib-diagnostic:v1.0.5
      imagePullPolicy: Always
      args:
      - |
        set -ex
        /scripts/container_entry.sh install --install-nccl
        cp -R /var/lib/gib/lib64/. /target/usr/local/gib/lib64
        cp -R /var/lib/gib/. /target/usr/local/gib
        cp -R /scripts /target/dev/shm
        cp -R /diagnostic /target/dev/shm
        cp -R /third_party /target/dev/shm
      command:
      - /bin/sh
      - -c
      volumeMounts:
      - mountPath: /target/usr/local/gib
        name: gib
      - mountPath: /target/dev/shm
        name: shared-memory